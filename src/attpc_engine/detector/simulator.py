from .parameters import Config
from .solver import generate_point_cloud
from .writer import SimulationWriter
from .constants import NUM_TB
from .. import nuclear_map
from .pairing import unpair
from .typed_dict import NumbaTypedDict

import numpy as np
import h5py as h5
from tqdm import trange
from numpy.random import default_rng, Generator
from pathlib import Path
from numba.typed import Dict
from numba.core import types
from numba import njit


@njit
def dict_to_points(
    points: NumbaTypedDict[int, tuple[int, int]],
) -> tuple[np.ndarray, np.ndarray]:
    """
    Converts dictionary of N pad,tb keys with corresponding number of electrons
    to Nx3 array where each row is [pad, tb, e], now combined over pad/tb combos.

    Parameters
    ----------
    points: numba.typed.Dict[int, int]
        A dictionary mapping a unique pad,tb key to the number of electrons.

    Returns
    -------
    tuple[numpy.ndarray, numpy.ndarray]
        Array of points and lables (in that order)
    """
    point_array = np.empty((len(points), 3), dtype=float)
    label_array = np.empty(len(points), dtype=types.int64)
    for idx, (key, data) in enumerate(points.items()):
        tb, pad = unpair(key)
        point_array[idx, 0] = pad
        point_array[idx, 1] = tb
        point_array[idx, 2] = data[0]
        label_array[idx] = data[1]

    return point_array, label_array


def simulate(
    momenta: np.ndarray,
    vertex: np.ndarray,
    proton_numbers: np.ndarray,
    mass_numbers: np.ndarray,
    config: Config,
    rng: Generator,
    indicies: list[int],
) -> tuple[np.ndarray, np.ndarray]:
    points = Dict.empty(
        key_type=types.int64, value_type=types.Tuple(types=[types.int64, types.int64])
    )
    for idx in indicies:
        if proton_numbers[idx] == 0:
            continue
        nucleus = nuclear_map.get_data(proton_numbers[idx], mass_numbers[idx])
        momentum = momenta[idx]
        generate_point_cloud(momentum, vertex, nucleus, config, rng, points, idx)

    # Convert to numpy array of [pad, tb, e], now combined over pad/tb combos
    point_array, label_array = dict_to_points(points)

    # Wiggle point TBs over interval [0.0, 1.0). This simulates effect of converting
    # the (in principle) int TBs to floats.
    point_array[:, 1] += rng.uniform(low=0.0, high=1.0, size=len(point_array))

    # Remove points outside legal bounds in time. TODO check if this is needed
    mask = np.logical_and(0 <= point_array[:, 1], point_array[:, 1] < NUM_TB)
    point_array = point_array[mask]
    label_array = label_array[mask]

    return point_array, label_array


def run_simulation(
    config: Config,
    input_path: Path,
    writer: SimulationWriter,
    indicies: list[int] | None = None,
):
    """Run the simulation

    Runs the AT-TPC simulation with the input parameters on the specified
    kinematic data hdf5 file generated by the kinematic pipeline.

    Parameters
     ----------
    config: Config
        The simulation configuration
    input_path: pathlib.Path
        Path to HDF5 file containing kinematics
    writer: SimulationWriter
        An object which implements the SimulationWriter Protocol
    """
    print("------- AT-TPC Simulation Engine -------")
    print(f"Applying detector effects to kinematics from file: {input_path}")
    input = h5.File(input_path, "r")
    input_data_group: h5.Group = input["data"]  # type: ignore
    proton_numbers: np.ndarray = input_data_group.attrs["proton_numbers"]  # type: ignore
    mass_numbers = input_data_group.attrs["mass_numbers"]

    # Decide which nuclei to sim, either by user input or all reaction final products
    nuclei_to_sim = None
    if indicies is not None:
        nuclei_to_sim = indicies
    else:
        # default nuclei to sim, all final outgoing particles
        nuclei_to_sim = [idx for idx in range(2, len(proton_numbers), 2)]
        nuclei_to_sim.append(len(proton_numbers) - 1)  # add the last

    n_events: int = input_data_group.attrs["n_events"]  # type: ignore
    miniters = int(0.01 * n_events)
    n_chunks: int = input_data_group.attrs["n_chunks"]  # type: ignore
    chunk_size: int = input_data_group.attrs["chunk_size"]  # type: ignore
    print(
        f"Found {n_events} kinematics events in {n_chunks} {chunk_size} event chunks."
    )
    print(f"Output will be written to {writer.get_directory_name()}.")

    rng = default_rng()

    print("Go!")
    # ASCII art edited from https://emojicombos.com/f1-car-text-art
    print(
        r"""
            ⠀⢀⣀⣀⣀⠀⠀⠀⠀⢀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⢸⣿⣿⡿⢀⣠⣴⣾⣿⣿⣿⣿⣇⡀⠀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
            ⠀⢸⣿⣿⠟⢋⡙⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣿⡿⠓⡐⠒⢶⣤⣄⡀⠀⠀
            ⠀⠸⠿⠇⢰⣿⣿⡆⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⣿⣿⡷⠈⣿⣿⣉⠁⠀
            ⠀⠀⠀⠀⠀⠈⠉⠀⠈⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠀⠈⠉⠁⠀⠈⠉⠉⠀⠀
        """
    )

    for event_number in trange(n_events, miniters=miniters):  # type: ignore
        chunk = event_number // chunk_size  # integer floor division
        dataset: h5.Dataset = input_data_group[f"chunk_{chunk}"][  # type: ignore
            f"event_{event_number}"
        ]  # type: ignore
        cloud, labels = simulate(
            dataset[:].copy(),  # type: ignore
            np.array(
                [
                    dataset.attrs["vertex_x"],
                    dataset.attrs["vertex_y"],
                    dataset.attrs["vertex_z"],
                ]
            ),
            proton_numbers,  # type: ignore
            mass_numbers,  # type: ignore
            config,
            rng,
            nuclei_to_sim,
        )

        if len(cloud) == 0:
            continue

        writer.write(cloud, labels, config, event_number)
    writer.close()
    print("Done.")
    print("----------------------------------------")
